# Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail

Alpamayo-R1 (AR1) に関する論文の要約です。本モデルは、推論（Reasoning）と行動予測（Action Prediction）を統合することで、ロングテールな状況における自動運転の汎化性能を向上させることを目的としています。

## アブストラクト (Abstract)
**内容まとめ**:
エンドツーエンドの自動運転モデルは規模の拡大により進歩しましたが、安全性に関わる稀な状況（ロングテール）では依然として脆さが残ります。本研究では、VLA（Vision-Language-Action）モデルである**Alpamayo-R1 (AR1)** を紹介します。主な革新点は以下の3点です：
1.  **Chain of Causation (CoC) データセット**: 意思決定に基づき、因果関係が明確な推論トレースを含むデータセットを構築。
2.  **モジュラーVLAアーキテクチャ**: 物理AI向けに事前学習されたVLM「Cosmos-Reason」と、リアルタイムで実行可能な拡散ベースの軌道デコーダを統合。
3.  **マルチステージトレーニング**: 教師あり微調整（SFT）で推論能力を引き出し、強化学習（RL）で推論と行動の一貫性および品質を最適化。
結果として、複雑なシナリオでの計画精度が12%向上し、シミュレーションでの近接遭遇率が35%減少しました。実車でも99msのレイテンシで動作を確認しています。

## 1. イントロダクション (Introduction)
**内容まとめ**:
自動運転システムはモジュラー型からEnd-to-End (E2E) 型へと移行していますが、現在のE2Eアプローチは、データが少ないロングテールな状況や高度な推論が必要な場面で苦戦しています。LLMの推論能力（Chain of Thoughtなど）はこの解決策として有望ですが、既存のVLAは推論を欠くか、自由形式で構造化されていないため、運転に必要な因果関係や構造的知識を見落としがちです。
本研究のAlpamayo-R1は、構造化された「Chain of Causation (CoC)」フレームワークを導入し、推論を具体的な運転判断と因果的に結びつけます。これにより、解釈可能性だけでなく、実際の運転性能（特に安全性）を向上させます。

## 2. 関連研究 (Related Work)
**内容まとめ**:
*   **2.1 自動運転におけるVLMとVLA**: 一般的なVLMを運転に活用する研究や、言語と行動を結合するVLA（例：OpenDriveVLA）がありますが、多くは明示的な推論を行わないか、反応的な動作にとどまっています。
*   **2.2 自動運転における推論VLA**: CoT（Chain of Thought）を用いた推論VLA（例：AdaThinkDrive, Poutine）が登場していますが、多くは自由形式の推論に依存しており、行動との因果的な整合性が不十分です。
*   **2.3 ポストトレーニングアライメント**: 生成モデルの出力を人間の意図に合わせるため、RLHF（Reinforcement Learning from Human Feedback）やRLVR（Verifiable Rewards）などの技術が重要です。本研究ではこれを推論の質と行動の整合性向上に応用します。
*   **2.4 自動運転のためのVision-Languageデータセット**: 既存のデータセット（nuScenes-QA, DriveLMなど）は知覚中心であったり、推論と行動の因果関係が曖昧であったり、「未来の情報を参照してしまう（因果的混乱）」問題を含んでいることがあります。

## 3. 推論VLAアーキテクチャの構築 (Building a Reasoning VLA Architecture)
**内容まとめ**:
AR1は、Alpamayo-VAを拡張したモジュラーVLAアーキテクチャです。
*   **3.1 VLMバックボーン: Cosmos-Reason**: 物理AI（Physical AI）向けに特別に事前学習されたVLM「Cosmos-Reason」を採用。Cosmos-Reason-7Bは[Qwen2.5-VLをベース](file:///home/masa/alpamayo/paper/cosmos-reason1/cosmos-reason1.txt#L461)としており、物理的な常識や運転シナリオの理解に優れています。
*   **3.2 ドメイン固有の適応**:
    *   **3.2.1 ビジョンエンコーディング**: マルチカメラ・マルチタイムステップの入力を効率的に処理するため、標準的なシングルイメージトークナイゼーションに加え、TriplaneやFlexといった効率的な手法もサポートし、トークン数を削減します。
    *   **3.2.2 軌道デコーディング**: ユニサイクルダイナミクスに基づく制御表現（加速度、曲率）を採用。推論時には、離散トークンではなく、フロー・マッチング（Flow Matching）に基づく「アクションエキスパート」を用いて、連続的かつマルチモーダルな軌道を高速に生成します。

**Alpamayo-R1-10B パラメータ内訳**:

| コンポーネント | パラメータ数 | 備考 |
|---------------|-------------|------|
| **Cosmos-Reason-7B** | ~7B | VLM全体 |
| - Vision Encoder (ViT) | 676M | [Cosmos-Reason1 Table 3](file:///home/masa/alpamayo/paper/cosmos-reason1/cosmos-reason1.txt#L527) |
| - Projector | （小） | 2層MLPダウンサンプリング |
| - LLM Backbone | ~6.3B | Transformer 28層 |
| **Diffusion Trajectory Decoder** | **2B** | フロー・マッチングベース |
| **合計** | ~9B | 「10B」は概算表記 |

## 4. Chain of Causation (CoC) データセット (Chain of Causation Dataset)
**内容まとめ**:
既存データの課題（曖昧さ、表層的な推論、因果的混乱）に対処するため、構造化されたラベリングフレームワークを開発しました。
*   **4.1 構造化されたChain of Causation**:
    1.  **運転意思決定 (Driving Decision)**: 定義済みのリスト（例：先行車追従、車線維持、一時停止など）から選択。
    2.  **重要な構成要素 (Critical Components)**: 意思決定に直接影響する因果要因（信号、障害物など）。
    3.  **構成されたCoCトレース**: 上記に基づき、論理的に構成された自然言語による理由説明。
*   **4.2 データキュレーション**: 意思決定が明確に関与する「重要な瞬間（キーフレーム）」のみを、リアクティブおよびプロアクティブなシナリオに基づいて選定します。
*   **4.3 ハイブリッドラベリング手順**:
    *   **人間によるラベリング**: 過去の履歴のみに基づいて因果要因を特定する厳格な手順で、高品質なデータを生成。
    *   **自動ラベリング**: **Qwen3-VL**等の教師VLMを用い、運転固有のプライオリ（縦方向・横方向・レーン関連のメタアクション、速度情報）をプロンプトとして大規模データを生成。この知識蒸留により、推論予測能力を強化。
    *   **評価**: LLMを用いた自動評価と人間による検証を組み合わせ、推論の因果関係の正確さを担保します。

## 5. トレーニング戦略 (Training Strategy)
**内容まとめ**:
*   **5.1 アクションモダリティの注入**: VLMに離散的な軌道トークンを入出力として学習させつつ、推論時にはフロー・マッチング・デコーダを使用するデュアル表現を採用。
*   **5.2 推論の引き出し (Eliciting Reasoning)**: CoCデータセットを用いた教師あり微調整（SFT）により、モデルに因果的な推論能力を模倣させます。しかし、SFTだけではデータのバイアスや行動との不一致が残ります。
*   **5.3 RLベースのポストトレーニング**:
    *   **アルゴリズム**: GRPO（Group Relative Policy Optimization）を採用。
    *   **報酬モデル**: 以下の3要素を最適化します。
        1.  **推論品質**: 大規模推論モデル（LRM）による評価。
        2.  **CoCと行動の一貫性**: 推論された意図と、生成された軌道のメタアクションが一致するかどうかのバイナリ報酬。
        3.  **低レベル軌道品質**: 専門家の軌道との類似性、衝突回避、滑らかさ（ジャーク抑制）。
    *   **データキュレーション**: モデルの内部予測と外部報酬の不一致が大きいサンプルを優先的に学習し、効率化を図ります。

## 6. 実験 (Experiments)
**内容まとめ**:
*   **6.1 評価プロトコル**: オープンループ（予測精度）、クローズドループ（AlpaSimシミュレーション）、アブレーション、実車テストを実施。
*   **6.2 推論によるポリシーの改善**: CoC推論を導入することで、特にルート情報がある場合や困難なシナリオにおいて、軌道予測精度（minADE）が向上し、シミュレーションでの近接遭遇率が低減しました。
*   **6.3 RLポストトレーニングによる改善**: SFTのみと比較して、RL後は推論の質が45%、推論と行動の一貫性が37%向上しました。SFTモデルで見られた「推論は正しいが行動が伴わない（例：止まると言いながら止まらない）」問題が解決されました。
*   **6.4 公開ベンチマーク評価**: PhysicalAI-AVデータセットなどにおいて、モデルサイズ（10B vs 0.5B）の効果を確認。大規模モデルの方が高い性能を示しました。
*   **6.5 アブレーション: VLMバックボーン**: モデルサイズ（0.5B < 3B < 7B）とデータ量のスケーリング則を確認。また、Cosmos-Reasonのような物理AI特化の事前学習の効果を実証しました。
*   **6.6 アブレーション: アクションモダリティ**: フロー・マッチングを用いることで、自己回帰的なトークン生成よりも精度、快適性、推論速度が優れていることを確認。
*   **6.7 アブレーション: 効率的なビジョンエンコーディング**: TriplaneやFlexを用いることで、性能を維持しつつトークン数を最大20倍削減可能であることを示しました。
*   **6.8 実車ロードテスト**: NVIDIA RTX 6000 Pro Blackwell上で99msというリアルタイム性能で動作し、実際の都市環境での自律走行に成功しました。

## 7. 結論 (Conclusion)
**内容まとめ**:
Alpamayo-R1は、構造化された推論と行動予測を統合することで、自動運転におけるロングテール問題に対処しました。成功の鍵は、因果関係を重視したCoCデータセットと、推論と行動の一貫性を高めるRLポストトレーニングにあります。
今後の展望として、階層的なポリシー構造、必要な時だけ推論を行う適応的なメカニズム、ワールドモデルの統合などが挙げられています。
