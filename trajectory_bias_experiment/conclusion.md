# 結論: 推論パラメータ調整の限界と今後の展望 (Conclusion)

## 1. 実験結果の総括 (Summary of Inference Tuning)
「直進バイアス」の解消を目指し、ユーザーが手軽に変更可能な4つの推論パラメータ（Temperature, Prompt, Num Samples, Top-P）について検証を行いました。

| パラメータ | 検証結果 |
|---|---|
| **Temperature** | 温度を上げても制御不能なノイズ（蛇行）が増えるのみで、滑らかなカーブ軌道への誘導には寄与しなかった。 |
| **Prompt** | CoT（思考）はカーブを認識するように矯正できたが、それがAction（軌道）に伝播せず、行動は直進のままであった。 |
| **Num Samples** | 生成数を増やすことで稀に「曲がる」軌道が出るが、確率は低く、実用的な解決策とは言えない。 |
| **Top-P** | 論文推奨値 (0.98) が最適であり、変更による改善効果は見られなかった。 |

## 2. 結論: パラメータ調整の限界 (Limitation)
**結論として、これらの推論パラメータ調整だけでは「直進バイアス」および「思考と行動の乖離」を解決することはできませんでした。**

この現象は、推論時の設定ミスではなく、**モデルの学習済み分布そのものが「直進」に強く偏っている（Mode Collapse）** ことに起因しています。特にプロンプトで思考を正しても行動が変わらない点は、モデル内部の Reasoning Head と Action Head の結合（Alignment）が学習段階で十分に獲得できていないことを示しています。

### 論文における言及 (Reference in Paper)
Alpamayo-R1の論文 (`arXiv:2511.00088`) においても、この **"Reasoning-Action Consistency"（思考と行動の一貫性）** は主要な技術課題として扱われています。
論文では、SFT（教師あり微調整）だけでは不十分であり、**RL（強化学習）を用いることで初めてこの一貫性が大幅に改善される（Consistency +37%）** ことが報告されています。
つまり、現状の挙動はバグではなく、追加学習（RL）が不足している状態における「仕様通りの限界」と言えます。

## 3. 次のステップ: Fine-tuning の必要性 (Future Work)
課題を根本的に解決するためには、推論時の小手先の調整ではなく、モデル自体を更新する **Fine-tuning** が不可欠です。

### 推奨アクション
*   **Cosmos Cookbook の活用**: 公式ツールチェーンを用いて、単眼カメラ画像を含むカスタムデータセット（思考と行動が一貫したカーブ走行データ）での **LoRA Fine-tuning** を実施する。
*   により、モデル内部のアライメントを物理的に更新し、思考を行動に反映させる能力を獲得させる必要があります。
