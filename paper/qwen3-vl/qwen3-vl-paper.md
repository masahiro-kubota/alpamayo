# Qwen3-VL Technical Report

Qwen3-VLに関する論文の要約です。本モデルは、Qwenシリーズにおける最高性能のビジョン・ランゲージモデルであり、テキスト、画像、動画を最大256Kトークンのコンテキストで統合的に処理できます。

## アブストラクト (Abstract)
**内容まとめ**:
本論文ではQwen3-VLを紹介します。これはQwenシリーズで最も高性能なビジョン・ランゲージモデル（VLM）であり、幅広いマルチモーダルベンチマークで優れた性能を発揮します。主な特徴は以下の3点です：
1.  **純粋なテキスト理解の大幅な強化**: 一部のケースでは同等のテキスト専用モデルを上回る性能を達成。
2.  **ロングコンテキスト対応**: テキストとマルチモーダル入力の両方に対して256Kトークンのネイティブウィンドウをサポートし、長い文書や動画にわたる忠実な保持・検索・相互参照を可能に。
3.  **高度なマルチモーダル推論**: 単一画像、複数画像、動画タスクにおいて、MMMUやMathVistaなどの包括的な評価で業界トップの性能を実現。
モデルはDense（2B/4B/8B/32B）とMoE（30B-A3B/235B-A22B）の両バリアントで提供され、レイテンシと品質のトレードオフに対応します。

## 1. イントロダクション (Introduction)
**内容まとめ**:
ビジョン・ランゲージモデル（VLM）は近年大きく進歩し、ロングコンテキスト理解、STEM推論、GUI操作、エージェントワークフローなど、多岐にわたる下流アプリケーションを生み出しています。重要なのは、マルチモーダル能力の向上が、基盤となる大規模言語モデル（LLM）の言語能力を損なわないことです。
本論文ではQwen3-VLを紹介し、Qwen3シリーズを基盤に、4つのDenseモデル（2B/4B/8B/32B）と2つのMoEモデル（30B-A3B / 235B-A22B）を提供します。全モデルは256Kトークンのコンテキストウィンドウで訓練されています。Non-thinking版とThinking版の両方をリリースし、後者は複雑な推論タスクで特に優れた性能を示します。

## 2. モデルアーキテクチャ (Model Architecture)
**内容まとめ**:
Qwen3-VLはQwen2.5-VLを踏襲し、ビジョンエンコーダ、MLPベースのビジョン-ランゲージマージャー、大規模言語モデル（LLM）の3モジュール構成を採用しています。

*   **2.1 Interleaved MRoPE**: Qwen2-VLで導入されたMRoPEを改良。時間(t)、水平(h)、垂直(w)の各次元を埋め込み次元全体に交互配置することで、不均衡な周波数スペクトルを解消し、長時間動画の理解を大幅に向上。
*   **2.2 DeepStack**: ビジョン-ランゲージの整合性を強化するため、DeepStackメカニズムを導入。ビジョンエンコーダの異なる層からの視覚トークンを、対応するLLM層に軽量な残差接続を通じて注入し、コンテキスト長を増やさずに多層融合を実現。
*   **2.3 Video Timestamp**: Qwen2.5-VLでの位置エンコーディングによる絶対時間整合を廃止し、フレームグループに明示的なタイムスタンプトークン（例：`<3.0 seconds>`）を付与。これによりモデルは時間情報をより効果的かつ正確に認識し、動画グラウンディングや密なキャプション付けなどのタスクで優れた性能を発揮。

## 3. 事前学習 (Pre-Training)
**内容まとめ**:
事前学習は4段階で構成されます：

*   **3.1 トレーニングレシピ**:
    *   **Stage 0 (Vision-Language Alignment)**: マージャー層のみを訓練（8Kシーケンス長、67Bトークン）。
    *   **Stage 1 (Multimodal Pre-Training)**: 全パラメータを解凍し、約1Tトークンで学習（8Kシーケンス長）。
    *   **Stage 2 (Long-Context Pre-Training)**: シーケンス長を32Kに拡張し、約1Tトークンで学習。
    *   **Stage 3 (Ultra-Long-Context Adaptation)**: シーケンス長を262Kに拡張し、100Bトークンで超長コンテキストに適応。
*   **3.2 事前学習データ**:
    *   **3.2.1 画像キャプションとインターリーブドデータ**: 高品質なキャプションペアとテキスト-画像インターリーブドシーケンスを大規模に拡張。
    *   **3.2.2 知識**: 動物、植物、ランドマーク、食品など10以上のセマンティックカテゴリにわたるエンティティ知識をカバー。
    *   **3.2.3 OCR、文書パース、長文書理解**: 39言語対応のOCR、700万以上のPDF文書パース、長文書VQAデータを構築。
    *   **3.2.4 グラウンディングとカウンティング**: ボックスベースおよびポイントベースのグラウンディングデータを構築。座標系を[0, 1000]に正規化。
    *   **3.2.5 空間理解と3D認識**: 空間関係、オブジェクトアフォーダンス、3Dグラウンディング（9-DoFバウンディングボックス）用のデータを構築。
    *   **3.2.6 コード**: Qwen3/Qwen3-Coderのテキストコーディングコーパスと、UI→HTML変換、SVG生成などのマルチモーダルコーディングデータ。
    *   **3.2.7 動画**: 時間認識動画理解（Dense Caption、Spatio-Temporal Grounding）とデータバランシング戦略を採用。
    *   **3.2.8 STEM**: 6000万以上のK-12・大学レベルの問題と、1200万以上のマルチモーダル推論サンプル。
    *   **3.2.9 エージェント**: GUI操作データ（デスクトップ/モバイル/ウェブ）、Function Calling軌跡、画像/テキスト検索ツール使用データ。

## 4. ポストトレーニング (Post-Training)
**内容まとめ**:
ポストトレーニングは3段階で構成されます：

*   **4.1 トレーニングレシピ**:
    *   **Supervised Fine-Tuning (SFT)**: 指示追従能力と推論スキルを活性化。32K→256Kのコンテキスト拡張を含む2フェーズ。Non-thinking版とThinking版（Chain-of-Thought形式）を分離。
    *   **Strong-to-Weak Distillation**: 強力な教師モデルから軽量な生徒モデルへの知識蒸留。テキストのみのデータでLLMバックボーンを微調整。
    *   **Reinforcement Learning (RL)**: 推論RL（数学、コーディング、視覚グラウンディングなど検証可能なタスク）と汎用RL（VQA、OCR、キャプション付けなど）を適用。SAPO アルゴリズムを使用。
*   **4.2 Cold Start Data**: 約120万サンプルのSFTデータセット。テキスト1/3、画像-テキスト・動画-テキスト2/3の構成。厳格なクエリフィルタリングとレスポンスフィルタリングを実施。
*   **4.3 Strong-to-Weak Distillation**: Off-policy蒸留（教師出力による学習）とOn-policy蒸留（生徒出力でのKL最小化）の2フェーズ。
*   **4.4 強化学習**: Reasoning RL（30K RLクエリ、SAPO使用）とGeneral RL（多タスク報酬による汎化向上）を実施。
*   **4.5 Thinking with Images**: 視覚エージェントとしての能力を付与。ツール使用を含むマルチターンRLを2段階で実施。答えの正確性、マルチターン推論、ツール呼び出しの3つの報酬を組み合わせ。

## 5. 評価 (Evaluation)
**内容まとめ**:
広範なベンチマークで評価を実施：

*   **5.1 一般的なVQA**: MMBench、RealWorldQA、MMStarなどで、Denseモデル・MoEモデル共に高水準の性能を達成。
*   **5.2 マルチモーダル推論**: MMMU、MathVista、MathVision、LogicVistaなどのSTEM・パズルベンチマークで業界トップレベル。Thinking版はMathVista、MathVisionでSOTA達成。
*   **5.3 アライメントと主観タスク**: HallusionBench、MM-MT-Bench、MIA-Benchで優れた指示追従能力とハルシネーション抑制を実証。
*   **5.4 テキスト認識と文書理解**: 39言語対応OCR、DocVQA、InfoVQA、ChartQAで高性能。MMLongBench-Docで長文書理解のSOTA達成。
*   **5.5 2D/3Dグラウンディング**: RefCOCO、ODinW-13、CountBenchでSOTA。Omni3Dベンチマーク（ARKitScenes、Hypersim、SUN RGB-D）で3Dグラウンディングも優秀。
*   **5.6 高精細知覚**: V*、HRBench-4k/8kでツール使用時にSOTA。ツール統合による性能向上がモデルサイズ増加より効果的。
*   **5.7 マルチイメージ理解**: BLINK、MuirBenchで複数画像間のパターン学習・マルチホップ推論で優れた性能。
*   **5.8 具現化・空間理解**: ERQA、VSI-Bench、EmbSpatial、RefSpatial、RoboSpatialHomeでトップクラス。
*   **5.9 動画理解**: VideoMME、MVBench、LVBench、MLVUで包括的な動画理解能力を実証。256Kトークン拡張により長時間動画でもGemini-2.5-Proを凌駕。
*   **5.10 エージェント**: ScreenSpot Pro、OSWorld、AndroidWorldでGUI知覚と意思決定能力のSOTA達成。32BモデルがOSWorldで41点、AndroidWorldで63.7点。
*   **5.11 テキスト中心タスク**: MMLU-Pro、AIME-25、LiveCodeBenchなどの純粋テキストベンチマークでも、テキスト専用LLMと同等以上の性能。視覚とテキストの能力統合を達成。
*   **5.12 アブレーション**: Qwen3-ViT（Vision Encoder改良）、DeepStack、Needle-in-a-Haystack（1Mトークンまで99.5%精度維持）の有効性を検証。

## 6. 結論 (Conclusion)
**内容まとめ**:
Qwen3-VLは、高品質なマルチモーダルデータ反復とアーキテクチャ革新（Interleaved MRoPE、DeepStack、テキストベースの時間グラウンディング）を統合し、マルチモーダル理解と生成の最前線を推進する最先端VLMファミリーです。256Kトークンのネイティブサポートにより、長くて複雑な文書・画像シーケンス・動画にわたる堅牢な推論が可能です。
将来的には、Qwen3-VLをデジタル世界と物理世界を橋渡しする具現化AIエージェントの基盤エンジンとして発展させることを構想しています。インタラクティブ知覚、ツール拡張推論、リアルタイムマルチモーダル制御への能力拡張を目指しています。モデルファミリー全体がApache 2.0ライセンスでオープンリリースされています。
